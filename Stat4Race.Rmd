---
title: "Stat4Race"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

Look at colab [link](https://colab.research.google.com/drive/1CMkKzX2K0-l90rAZXl9V1RST_YEOKAzr?usp=sharing)

I will transfer this here later, but the performances must be measured on colab nonetheless.

## Simulation function:
Given as input a population size `M`, the `sim` function simulates `M` times $T \sim |\{Y_1..Y_n, \ Y_n > X\}|$. 
At first it generates a uniform vector of size $M$, which will be $X$, then it samples $M \times b$ uniform values and puts them in a matrix, where $b$ is the base step. At this iteration $b$ values are generated for each of the $M$ individuals in the population, and if the $j$-th individual contains a value major than $X$ in the vector $M_{j,k} > X \ \ \forall k \in \{1..b\}$ then the the first k in which this happens is taken and saved in the vector `res`. 
Then the process is repeated for each iteration $i$ for all those values that still haven't found a solution but this time with an increase of the samples extracted for each variable. So if the number of individuals that now found a solution is $M_i$ at the i-th step: 
1. We sample a random uniform $(M-M_i) \times b^i$ matrix.
2. We check which of the individuals has terminated its course.
3. If there are no more survivors we stop the execution, otherwise we continue with the remaining $M_{i+1}$.
```{r}
sim <- function(M = 1, steps.base=10){
    X = runif(M)# We pick M random X
    res = integer(M)
    miss.vars = 1:M
    miss.n = M
    i = 1
    steps.prev = 0
    while(miss.n != 0){
        #if (i>1) 
        #    print(paste('Remaining r.v. are:', miss.n))
        # at the i-th iteration we pick l^(i+1) values for each of the remaining rows
        steps.now = min(steps.base^i, 1e6)
        y <- matrix(
            runif(steps.now * miss.n), 
            nrow = miss.n)
        trespass = y>(X[miss.vars])                   # for each row we see which value is over its X
        over = ifelse(
            rowSums(trespass)==0,                   # if no trespasses for that row
            0,                                      # then it's 0
            steps.prev + max.col(trespass, "first"))   # else it's the first of the trespasses
        res[miss.vars] = res[miss.vars] + over          # we add up the result with the partial we just obtained

        miss.vars = which(res==0)                     # we update the missing values
        miss.n = length(miss.vars)                     
        steps.prev = steps.prev + steps.now
        i = i + 1
    }
    return(res)
}
sim(100)
```
If now want to analyze this function we can notice a few things:
- at the beginning only a few of variables is extracted for each individual, this is because most of the individuals are short lived and will be gone after no more than 10 steps.
- as the iterations go on the number of steps for each iteration increases exponentially $b^i$, this is because while with a large population we were restricted in the size of the steps due to memory limitations, when the population tends to decrease exponentially, we can exponentially increase the size of samples per iteration.
- the population surviving to the next iteration is actually a bad event for our algorithm, as we have to resort to use the while loop. That's also why we want to increase the size of samples per iteration $b^i$, as doing so will guarantee us a faster extermination of our population.
- by doing some empirical testing we can see that the number of iterations we have per each function call is usually in the order of $O(log(M))$


We can now confront the output of our simulation with the $pdf$ of T
```{r}
T_var = sim(10000)
hist(T_var, breaks = 10000, xlim=c(1,50), prob=T)
curve(1/x/(x+1), col='orange', lwd=3, add=T)
```
We install and import some libraries for parallel processing, as we'll need them later:
```{r}
install.packages('foreach')
install.packages('doParallel')

library(foreach)
library(iterators)
library(doParallel) 
registerDoParallel(cores = detectCores())
```

Finally we compute the how much does it take, note that we split everything into 10 threads to speed up computing:
```{r}
set.seed(13112221)

M = 10^(2:7)
n.threads = 10
df = data.frame()
values = list()
for(m in M) {
    beg <- Sys.time()
    X = runif(m)
    r = foreach(idx=1:n.threads, .combine=append) %dopar% {
        sim(m/n.threads)
    }
    fin<- Sys.time() - beg
    print(fin)
    values = append(values, list(r))
    df = rbind(df, data.frame(size=m, time=fin))
}
df[c('size', 'time')]
```

This is the time it takes, vs some linear function:
```{r}
plot(
    df[c('size', 'time')],
    type = "l", 
    log='x',
    lwd=5, 
    col='orchid', 
    xlab='Population Size', 
    ylab='time [s]', 
    main='Time per size of input')
curve(x/1e5, add=T, lwd=1, type='p', from=min(df$size), to=max(df$size))
```

And this is the mean of the values, and as we can expect (since it has $\infty$ expected value) it tends to grow as the size of the simulation grows:
```{r}
df$mean = sapply(values, mean)
plot(
    df[c('size', 'mean')],
    type = "l", 
    log='x',
    lwd=5, 
    col='green', 
    xlab='Population Size', 
    ylab='Mean', 
    main='Mean per size of input')

```


