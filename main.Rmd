---
title: "main"
output: html_document
date: "2022-11-20"
author: "Jonas Barth, Mattia Castaldo, Matteo Migliarino"
---

# Index

-   [Stat4Race](#1.-stat4race)
-   [Differential Privacy](#2.-differential-privacy)
    -   [Univariate Differential Privacy](#2.1.-univariate-differential-privacy)
    -   [Sampling from a different distribution](#2.2.-sampling-from-a-different-distribution)
    -   [Privatised Dataset](#2.3.-privatised-dataset)
    -   [Bonus Question](#2.4.-bonus-question)

# 2. Differential Privacy {#2.-differential-privacy}

# 2.1. Univariate Differential Privacy {#2.1.-univariate-differential-privacy}

Index

-   [Requirements](#requirements)
-   [Original and Perturbed Histogram Example](#original-and-perturbed-histogram-example)
-   [Simulation](#simulation)
-   [Plotting](#plotting)

## Requirements {#requirements}

```{r}
require('VGAM') # for sampling from a laplace
require("dplyr") # for using group by on the data from the simulation
require("colorspace") # for plotting
require("latex2exp") # for mathematical expressions in plots
```

## Original and Perturbed Histogram Example {#original-and-perturbed-histogram-example}

To get a visual understanding of what it means to perturbe a histogram, we will show an original histogram sampled from a $Beta$ distribution and its perturbed version.

### Hyperparameters

```{r}
n = 1000
eps = 0.1
m = n^(1 / (2 + 1))
```

### Original Histogram

The original histogram is sampled from a $Beta$ distribution with $\alpha = 10, \beta = 10$.

```{r}
b_sample = rbeta(n=n, 10, 10)

original_hist = hist(b_sample, plot=F, breaks=m-1)

plot(original_hist,
     xlab = NULL,
     ylab = NULL,
     main = paste("Histogram of", n ,"samples from a Beta(10, 10) distribution"))
```

### Perturbed Histogram {#perturbed-histogram}

Since we have to create many perturbed histograms, we will put this logic into a function for reusability.

```{r}
l_perturbe = function(h, variance = 8 / (0.001 ^ 2)) {
    perturbed_hist = h
    lap_sample = rlaplace(length(perturbed_hist$counts), scale = sqrt(variance/2))
    
    # Add the laplace sample to D_j
    perturbed_hist$counts = perturbed_hist$counts + lap_sample
    
    # If we end up with a negative number, we choose 0 instead. Same as max(0, D_j)
    perturbed_hist$counts[perturbed_hist$counts <= 0] = 0
    
    # If our samples from the laplace lead to a negative count in each bin, we cannot normalise over the
    # sum since this would be a division over 0.
    if (any(is.na(perturbed_hist$counts))) {
        perturbed_hist$counts = rep(0, length(perturbed_hist$counts))
    } else {
        # Normalise D_j over the sum of all bins
        perturbed_hist$counts = perturbed_hist$counts / sum(perturbed_hist$counts)
    }

    # Also update the density, divide q_hat by 1/m
    perturbed_hist$density = perturbed_hist$counts / (1 / m)
    
    return(perturbed_hist)
}
```

Plot the peturbed histogram. We can see that it is more sparse than the original histogram.

```{r}
perturbed_hist = l_perturbe(original_hist, variance = 8 / (eps^2))
plot(perturbed_hist,
     freq=F,
     main='Perturbed Histogram')
```

## Simulation {#simulation}

Now we can get to the actual simulation of the $MISE$ between the original distribution and original histogram, and the $MISE$ between the original distribution and the perturbed histogram.

### Hyperparameters

These are the hyperparameters needed for the simulation.

```{r}
n_values = c(100,1000) # sample size
eps_values = c(0.1, 0.001) # values for epsilon
bins_values = seq(5, 50, 5) # values for the histogram bin size
M = 1000 # number of simulations to be run
```

### Function for running simulations with fixed hyperparameters

At each simulation, we do the following:

1.  Sample from the beta distribution.
2.  Create a histogram from the sample.
3.  Perturb the histogram.
4.  Calculate the $MISE(p_X, \hat{p}_{n, m})$.
5.  Calculate the $MISE(p_X, \hat{q}_{\epsilon, m})$.
6.  Save the $MISE$ values and hyperparameters of this simulation run into a dataframe.

```{r}
mise_sim = function(n, n_bins, eps, sim_size, dist=function(n) rbeta(n, 10, 10)) {
    
    out = data.frame()
    
    for (m in 1:sim_size) {

        # 1. sample from beta
        b_sample = dist(n=n)

        # 2. create original hist
        original_hist = hist(b_sample, plot=F, breaks=n_bins - 1)
        
        d_original = stepfun(original_hist$breaks, c(0, original_hist$density, 0))

        # 3. create perturbed hist
        perturbed_hist = l_perturbe(original_hist, variance = 8 / (eps^2))
        
        d_perturbed = stepfun(perturbed_hist$breaks, c(0, perturbed_hist$density, 0))
        
        # 4. calculate mise original
        mise_original = integrate(function(x) (dbeta(x, 10, 10) - d_original(x))^2, 0, 1, subdivisions = 1000)$value

        # 5. calculate mise perturbed
        mise_perturbed = integrate(function(x) (dbeta(x, 10, 10) - d_perturbed(x))^2, 0, 1, subdivisions = 1000)$value
        
        # 6. save the MISE values and hyperparameters
        out = rbind(out, data.frame(m=m, n=n, eps=eps, bins=n_bins, mise_original=mise_original, mise_perturbed=mise_perturbed))
    }
    
    return(out)
}

```

### Running the Simulation

We run the simulation $M$ times for all possible hyperparameter combinations. The results are saved into a dataframe.

```{r}
sim_data = data.frame()

# Create a grid with all possible hyperparameter combinations
hyperparameter_grid = expand.grid(n = n_values, bins = bins_values, eps = eps_values)

for(rowname in rownames(hyperparameter_grid)) {
    n = hyperparameter_grid[rowname, "n"]
    bins = hyperparameter_grid[rowname, "bins"]
    eps = hyperparameter_grid[rowname, "eps"]
    
    sim_result = mise_sim(n = n, n_bins = bins, eps = eps, sim_size = M)
    sim_data = rbind(sim_data, sim_result)
}
```

```{r}
sim_data
```

## Plotting {#plotting}

### Preparing Data for Plotting
We want to group the data by epsilon value, sample size (n), and number of bins, so that we can plot the MISE values as a function of the number of bins.

```{r}

# group data
mise_data_group <- sim_data %>%
  group_by(eps, n, bins) %>%
  dplyr::summarize(mise_original_mean = mean(mise_original),
                   mise_original_min = min(mise_original),
                   mise_original_max = max(mise_original),
                   mise_perturbed_mean = mean(mise_perturbed),
                   mise_perturbed_min = min(mise_perturbed),
                   mise_perturbed_max = max(mise_perturbed)) %>% 
  as.data.frame()
```

### Function for a single plot

This function plots a line plot with two lines:

-   mean of the original MISE.
-   mean of the perturbed MISE.

```{r}
plot_mise_values = function(mise_data, main, sub, colours, lwd = 3) {
  
  max_y = max(c(mise_data$mise_original_mean, mise_data$mise_perturbed_mean))
  plot(NULL,
     xlim = c(min(bins_values), max(bins_values)),
     ylim = c(0, max_y),
     ylab = "MISE",
     xlab = "Number of bins",
     main = main,
     sub = sub)
  lines(mise_data$bins, mise_data$mise_original_mean, type = "l", lwd = lwd, col = colours[1])
  lines(mise_data$bins, mise_data$mise_perturbed_mean, type = "l", lwd = lwd, col = colours[2])
}
```

### Plotting

The 4 plots below show the:

-   $MISE$ value for the original histogram.
-   $MISE$ value for the perturbed histogram.

for the 4 combinations of the sample sizes $n \: \in \: \{100, 1000}$ and the epsilon values $\epsilon \: \in \{0.001, 0.1\}$.

For a small privacy ratio $\epsilon = 0.001$, we can see that the $MISE$ of the perturbed histogram initially has a higher value that for a privacy ratio of $\epsilon = 0.1$. Also, a larger the sample size $n$ for the perturbed $MISE$ also corresponds to a higher initial $MISE$ when comparing values that have the same value for $\epsilon$. E.g. For $\epsilon = 0.001$, the perturbed $MISE$ for the sample size $n = 100$ starts at around $6$, whereas the perturbed $MISE$ for the sample size $n = 1000$ starts at around $10$. For small sample sizes $n = 100$, both the original $MISE$ starts off close to $0$, however with an increase in the number of bins, it approaches $1$.

One thing that is common across all 4 plots is that there is a decrease in the perturbed $MISE$ when moving from $5$ to $10$ bins. For $MISE$ that have $\epsilon = 0.1$, this initial decrease is then followed by a steady increase.

```{r}

plot_mise_lines = function(mise_data, n_values, eps_values, pi = NULL) {
  line_colours = rainbow_hcl(2)
 
  mise_data_1 = subset(mise_data, mise_data$n == n_values[1] & mise_data$eps == eps_values[1])
  mise_data_2 = subset(mise_data, mise_data$n == n_values[1] & mise_data$eps == eps_values[2])
  mise_data_3 = subset(mise_data, mise_data$n == n_values[2] & mise_data$eps == eps_values[1])
  mise_data_4 = subset(mise_data, mise_data$n == n_values[2] & mise_data$eps == eps_values[2])

  # Create a layout for the 4 plots and the common legend
  layout(matrix(c(1,2,5,3,4,5), ncol=2, nrow=3), heights=c(6, 6, 2))

  main_title = paste("MISE values over", M, "Simulations.")
  subtitle1 = paste("n = ", n_values[1], "eps = ", eps_values[1])
  subtitle2 = paste("n = ", n_values[1], "eps = ", eps_values[2])
  subtitle3 = paste("n = ", n_values[2], "eps = ", eps_values[1])
  subtitle4 = paste("n = ", n_values[2], "eps = ", eps_values[2])
  # Plot the 4 plots
  if (!is.null(pi)) {
      subtitle1 = paste(subtitle1, "pi = ", pi)
      subtitle2 = paste(subtitle2, "pi = ", pi)
      subtitle3 = paste(subtitle3, "pi = ", pi)
      subtitle4 = paste(subtitle4, "pi = ", pi)
  }
  
  subtitles = c(subtitle1, subtitle2, subtitle3, subtitle4)

  plot_mise_values(mise_data_1, main_title, subtitles[1], line_colours)
  plot_mise_values(mise_data_2, main_title, subtitles[2], line_colours)
  plot_mise_values(mise_data_3, main_title, subtitles[3], line_colours)
  plot_mise_values(mise_data_4, main_title, subtitles[4], line_colours)

  # Plot the legend
  par(mai=c(0,0,0,0))
  plot.new()
  legend("center",
         legend = c(TeX("$MISE(p_X, \\hat{p}_{n,m})$"), TeX("$MISE(p_X, \\hat{q}_{\\epsilon, m})$")),
         col = line_colours,
         lwd = 3,
         xpd = TRUE,
         horiz = TRUE,
         cex = 1,
         seg.len=2)
}

plot_mise_lines(mise_data_group, n = n_values, eps = eps_values)
```

# 2.2 Sampling from a different distribution

## Index

-   [Perturbed Histogram](#perturbed-histogram)
-   [Run Simulation](#run-simulation)
-   [Plotting](#2.2.-plotting)

We now sample from: $$
p_X(x) = \pi \cdot \text{dbeta}(x | \alpha_1, \beta_1) + (1 -\pi )\cdot \text{dbeta}(x | \alpha_2, \beta_2)
$$

```{r}
func = function(n, pi =.5) pi * rbeta(n,.5,.3) + (1 - pi) * rbeta(n,.5,.1)
h=hist(func(1000), plot=F)
plot(h, main='Density of
     p[X]',xlab = '', freq=F)
```

## Perturbed Histogram

```{r}
h_perturbed=l_perturbe(h)

plot(h_perturbed, main='Perturbed Histogram of p[X]', xlab='', freq=F)
```

## Run Simulation {#run-simulation}

We will run the simulation with the same parameters for $n$, $\epsilon$ and $m$ and for the same number of times $M$. We also have $\pi \; \epsilon \; \{0.25, 0.5, 0.75\}$ as an additional parameter.

```{r}
pi_values = c(.25,.5,.75)
out_new = data.frame()

hyperparameter_grid = expand.grid(n = n_values, bins = bins_values, eps = eps_values, pi = pi_values)

for (n in n_values) {
    for (eps in eps_values) {
        for (n_bins in bins_values) {
            for (pi in pi_values)   {
                sim_result = mise_sim(n, n_bins, eps, sim_size = M, dist=func)
                sim_result$pi = pi
                out_new = rbind(out_new, sim_result)
            }
        }
    }
}
```

## Plotting {#2.2.-plotting}

Like in the plots for question 2.1, we will show how the two calculated $MISE$ values vary across the different combinations of hyper parameters.

### Group Data

First, we group the data, however this time we also group on $\pi$ in addition to the other parameters.

```{r}
# group data
mise_data_group_pi <- out_new %>%
  group_by(eps, n, bins, pi) %>%
  dplyr::summarize(mise_original_mean = mean(mise_original),
                   mise_original_min = min(mise_original),
                   mise_original_max = max(mise_original),
                   mise_perturbed_mean = mean(mise_perturbed),
                   mise_perturbed_min = min(mise_perturbed),
                   mise_perturbed_max = max(mise_perturbed)) %>% 
  as.data.frame()
```

### Subset Data

Finally, we create one subset of the data for each value of $\pi$. We do this because splitting the data across three plots, each with their own $\pi$, makes the visualisation more readable.

```{r}
pi_025 = subset(mise_data_group_pi, mise_data_group_pi$pi == 0.25)
pi_05 = subset(mise_data_group_pi, mise_data_group_pi$pi == 0.5)
pi_075 = subset(mise_data_group_pi, mise_data_group_pi$pi == 0.75)
```

### Plot pi = 0.25.

```{r}
plot_mise_lines(pi_025, n_values, eps_values, pi = 0.25)
```

### Plot pi = 0.5.

```{r}
plot_mise_lines(pi_05, n_values, eps_values, pi = 0.5)
```

### Plot pi = 0.75.

```{r}
plot_mise_lines(pi_075, n_values, eps_values, pi = 0.75)
```

## 2.4 Bonus

We want to perform some simulations to test whether our mechanism ensures $\epsilon$ differential privacy:

```{r}
n.sim = 100
ratio = numeric(n.sim)
eps = .1

for (i in 1:n.sim) {
  h = hist(rbeta(100000,10,10), breaks=10,plot=F)
  h$density = h$density/2
  h_p = l_perturbe(h,variance=8/eps^2)

  diff = max( h$density[h_p$density!=0]/h_p$density[h_p$density!=0] )
  #plot(h, freq=F, c=rgb(1,0,0))
  #plot(h_p, freq=F, add=T, c= rgb(0,1,0,0.5))
  ratio[i] = diff
}
sum(ratio <= exp(eps)) / n.sim
```

#### TODO : for some reasone it appears that differential privacy isn't always ensured?
