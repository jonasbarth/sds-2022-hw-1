---
title: "main"
output: html_document
date: "2022-11-20"
author: "Jonas Barth, Mattia Castaldo, Matteo Migliarino"
---

# 2. Differential Privacy

# 2.1 Univariate Differential Privacy

Index

-   [Requirements](#requirements)
-   [Original and Perturbed Histogram Example](#original-and-perturbed-histogram-example)
-   [Simulation](#simulation)
-   [Plotting](#plotting)

## Requirements {#requirements}

```{r}
require('VGAM') # for sampling from a laplace
require("dplyr") # for using group by on the data from the simulation
require("colorspace") # for plotting
require("latex2exp") # for mathematical expressions in plots
```

## Original and Perturbed Histogram Example {#original-and-perturbed-histogram-example}

To get a visual understanding of what it means to perturbe a histogram, we will show an original histogram sampled from a $Beta$ distribution and its perturbed version.

### Hyperparameters

```{r}
n = 1000
eps = 0.1
m = n^(1 / (2 + 1))
```

### Original Histogram

The original histogram is sampled from a $Beta$ distribution with $\alpha = 10, \beta = 10$.

```{r}
b_sample = rbeta(n=n, 10, 10)

original_hist = hist(b_sample, plot=F, breaks=m-1)

plot(original_hist,
     xlab = NULL,
     ylab = NULL,
     main = paste("Histogram of", n ,"samples from a Beta distribution"))
```

### Perturbed Histogram
Since we have to create many perturbed histograms, we will put this logic into a function for reusability.
```{r}
l_perturbe = function(h, l_mean = 0, l_scale = 8 / (0.001 ^ 2)) {
    perturbed_hist = h
    lap_sample = rlaplace(length(perturbed_hist$counts), location = l_mean, scale = l_scale)
    
    # Add the laplace sample to D_j
    perturbed_hist$counts = perturbed_hist$counts + lap_sample
    
    # If we end up with a negative number, we choose 0 instead. Same as max(0, D_j)
    perturbed_hist$counts[perturbed_hist$counts <= 0] = 0
    
    # If our samples from the laplace lead to a negative count in each bin, we cannot normalise over the
    # sum since this would be a division over 0.
    if (any(is.na(perturbed_hist$counts))) {
        perturbed_hist$counts = rep(0, length(perturbed_hist$counts))
    } else {
        # Normalise D_j over the sum of all bins
        perturbed_hist$counts = perturbed_hist$counts / sum(perturbed_hist$counts)
    }

    # Also update the density, divide q_hat by 1/m
    perturbed_hist$density = perturbed_hist$counts / (1 / m)
    
    return(perturbed_hist)
}
```

Plot the peturbed histogram. We can see that it is more sparse than the original histogram.
```{r}
perturbed_hist = l_perturbe(original_hist, l_scale = 8 / (eps^2))
plot(perturbed_hist)

```

## Simulation {#simulation}
Now we can get to the actual simulation of the $MISE$ between the original distribution and original histogram, and the $MISE$ between the original distribution and the perturbed histogram.

### Hyperparameters

These are the hyperparameters needed for the simulation.

```{r}
n_values = c(100,1000) # sample size
eps_values = c(0.1, 0.001)
bins_values = seq(5, 50, 5)
M = 1000 # number of simulations to be run
```

### Function for running simulations with fixed hyperparameters

At each simulation, we do the following:

1.  Sample from the beta distribution.
2.  Create a histogram from the sample.
3.  Perturb the histogram.
4.  Calculate the $MISE(p_X, \hat{p}_{n, m})$.
5.  Calculate the $MISE(p_X, \hat{q}_{\epsilon, m})$.
6.  Save the $MISE$ values and hyperparameters of this simulation run into a dataframe.

```{r}
mise_sim = function(n, n_bins, eps, sim_size, dist=function(n) rbeta(n, 10, 10)) {
    
    out = data.frame()
    
    for (m in 1:sim_size) {

        # 1. sample from beta
        b_sample = dist(n=n)

        # 2. create original hist
        original_hist = hist(b_sample, plot=F, breaks=n_bins - 1)
        
        d_original = stepfun(original_hist$breaks, c(0, original_hist$density, 0))

        # 3. create perturbed hist
        perturbed_hist = l_perturbe(original_hist, l_scale = 8 / (eps^2))
        
        d_perturbed = stepfun(perturbed_hist$breaks, c(0, perturbed_hist$density, 0))
        
        # 4. calculate mise original
        mise_original = integrate(function(x) (dbeta(x, 10, 10) - d_original(x))^2, 0, 1, subdivisions = 1000)$value

        # 5. calculate mise perturbed
        mise_perturbed = integrate(function(x) (dbeta(x, 10, 10) - d_perturbed(x))^2, 0, 1, subdivisions = 1000)$value
        
        # 6. save the MISE values and hyperparameters
        out = rbind(out, data.frame(m=m, n=n, eps=eps, bins=n_bins, mise_original=mise_original, mise_perturbed=mise_perturbed))
    }
    
    return(out)
}

```

### Running the Simulation

We run the simulation $M$ times for all possible hyperparameter combinations. The results are saved into a dataframe.

```{r}
sim_data = data.frame()

# Create a grid with all possible hyperparameter combinations
hyperparameter_grid = expand.grid(n = n_values, bins = bins_values, eps = eps_values)

for(rowname in rownames(hyperparameter_grid)) {
    n = hyperparameter_grid[rowname, "n"]
    bins = hyperparameter_grid[rowname, "bins"]
    eps = hyperparameter_grid[rowname, "eps"]
    
    sim_result = mise_sim(n = n, n_bins = bins, eps = eps, sim_size = M)
    sim_data = rbind(sim_data, sim_result)
}
```

```{r}
sim_data
```

## Plotting {#plotting}

### Absolute MISE Differences

We want to show how the absolute difference between the two calculated $MISE$ values varies across the different combinations of hyper parameters.

#### Preparing Data for Plotting

There are 3 steps in the data preparation process:

1.  Calculate the absolute difference between the two $MISE$ values.
2.  Group the data by epsilon value, sample size (n), and number of bins.
3.  Get a subset for each possible (sample size, epsilon) pair. Four in total.

```{r}

# 1. absolute difference
sim_data$mise_diff = abs(sim_data$mise_original - sim_data$mise_perturbed)

# 2. group data
data_group <- sim_data %>%                                 
  group_by(eps, n, bins) %>%
  dplyr::summarize(mise_diff_mean = mean(mise_diff), 
                   mise_diff_min = min(mise_diff), 
                   mise_diff_max = max(mise_diff),
                   mise_original_mean = mean(mise_original),
                   mise_original_min = min(mise_original),
                   mise_original_max = max(mise_original),
                   mise_perturbed_mean = mean(mise_perturbed),
                   mise_perturbed_min = min(mise_perturbed),
                   mise_perturbed_max = max(mise_perturbed)) %>% 
  as.data.frame()
data_group


# 3. Get subsets.
data1 = subset(data_group, data_group$n == 100 & data_group$eps == 0.1)
data2 = subset(data_group, data_group$n == 100 & data_group$eps == 0.001)
data3 = subset(data_group, data_group$n == 1000 & data_group$eps == 0.1)
data4 = subset(data_group, data_group$n == 1000 & data_group$eps == 0.001)
```

#### Parameters for plotting

```{r}
colours = rainbow_hcl(4)
lwd = 4
legend_names = c("n = 100, eps = 0.1", "n = 100, eps = 0.001", "n = 1000, eps = 0.1", "n = 1000, eps = 0.001")
```

#### Plot for MISE Differences

The plot below shows the absolute differences between the MISE value for the original histogram and the MISE value for the perturbed histogram.

$$|MISE(p_X, \hat{p}_{n,m}) - MISE(p_X, \hat{q}_{\epsilon, m})|$$

Each line in the plot represents the absolute MISE difference for a specific combination of the number of samples from the Beta distribution $n$ and the privacy value epsilon $\epsilon$, averaged over $M$ simulations.

From the plot we can see that, as the number of bins increase, the MISE difference convergence seems to happen independently of the value for epsilon. For the smaller sample size $n = 100$, this happens around a number of bins $m = 25$. For the larger sample size $n = 1000$, this happens earlier around the number of bins $m = 10$.

This could be explained by the fact that smaller sample sizes are less likely to capture the necessary variance which is more likely to result in a skewed histogram when it has few bins.

```{r}
plot(NULL,
     xlim = c(min(bins_values), max(bins_values)),
     ylim = c(min(data_group$mise_diff_mean), max(data_group$mise_diff_mean)),
     ylab = "Mean MISE Differences",
     xlab = "Number of bins",
     main = paste("Mean MISE differences over", M, "simulations"))
lines(data1$bins, data1$mise_diff_mean, type = "l", lwd = lwd, col = colours[1])
lines(data2$bins, data2$mise_diff_mean, type = "l", lwd = lwd, col = colours[2])
lines(data3$bins, data3$mise_diff_mean, type = "l", lwd = lwd, col = colours[3])
lines(data4$bins, data4$mise_diff_mean, type = "l", lwd = lwd, col = colours[4])
legend("topright", 
       legend=legend_names,
       col = colours,
       lwd = lwd)
```

### Original Mise Variation

In these 4 plots we see the average: \* MISE value for the original histogram. \* MISE value for the perturbed histogram. \* Absolute difference between the MISE for the original histogram and MISE for the perturbed histogram.

The absolute differences shown in the 4 plots correspond to the lines in the previous plot. By also plotting the MISE values for the original and perturbed histogram, we can compare them easier than if we had just plotted the absolute difference. For example, for a sample size $n = 1000$, we can see that the original MISE is close to $0$ whereas the MISE of the perturbed stabilises around $2$.

```{r}
line_colours = rainbow_hcl(3)

plot_mise_values = function(mise_data, main, sub, colours) {
  
  max_y = max(c(mise_data$mise_original_mean, mise_data$mise_perturbed_mean, mise_data$mise_diff_mean))
  plot(NULL,
     xlim = c(min(bins_values), max(bins_values)),
     ylim = c(0, max_y),
     ylab = "MISE",
     xlab = "Number of bins",
     main = main,
     sub = sub)
  lines(mise_data$bins, mise_data$mise_original_mean, type = "l", lwd = 2, col = colours[1])
  lines(mise_data$bins, mise_data$mise_perturbed_mean, type = "l", lwd = 2, col = colours[2])
  lines(mise_data$bins, mise_data$mise_diff_mean, type = "l", lwd = 2, col = colours[3])
}

par(mfrow=c(2, 2))
plot_mise_values(data1, paste("MISE values over", M, "Simulations."), "n = 100, eps = 0.1.", line_colours)
plot_mise_values(data2, paste("MISE values over", M, "Simulations."), "n = 100, eps = 0.001.", line_colours)
plot_mise_values(data3, paste("MISE values over", M, "Simulations."), "n = 1000, eps = 0.1", line_colours)
plot_mise_values(data4, paste("MISE values over", M, "Simulations."), "n = 1000, eps = 0.001.", line_colours)
?par

legend("bottom",
         legend = c(TeX("$MISE(p_X, \\hat{p}_{n,m})$"), TeX("$MISE(p_X, \\hat{q}_{\\epsilon, m})$"), TeX("$|(MISE(p_X, \\hat{p}_{n,m}) - MISE(p_X, \\hat{q}_{\\epsilon, m})|$")),
          col = colours,
         lwd = 2, xpd = TRUE, horiz = TRUE, cex = 1, seg.len=2)

```

# 2.2 Samplng from a different distribution

We now sample from: $$
p_X(x) = \pi \cdot \text{dbeta}(x | \alpha_1, \beta_1) + (1 -\pi )\cdot \text{dbeta}(x | \alpha_2, \beta_2)
$$

```{r}
func = function(n, pi =.5) pi * rbeta(n,.5,.3) + (1 - pi) * rbeta(n,.5,.1)
h=hist(func(1000), plot=F)
plot(h, main='Density of p[X]',xlab = '')
```

```{r}
h_perturbed=l_perturbe(h)

plot(h_perturbed, main='Perturbed Histogram of p[X]', xlab='')
```

```{r}
pi_values<-c(.25,.5,.75)
out_new = data.frame()


for (n in n_values) {
    for (eps in eps_values) {
        for (n_bins in bins_values) {
            for (pi in pi_values)   {
                sim_result = mise_sim(n, n_bins, eps, sim_size = M, dist=func)
                out_new = rbind(out_new, sim_result)
            }
        }
    }
}

out_new
```
