---
title: "Stat4DS Homework#1"
output: html_document
date: "2022-11-20"
author: "Jonas Barth, Mattia Castaldo, Matteo Migliarini"
---

# Index

-   [Requirements](#requirements)
-   [Stat4Race](#1.-stat4race)
-   [Differential Privacy](#2.-differential-privacy)
    -   [Univariate Differential Privacy](#2.1.-univariate-differential-privacy)
    -   [Sampling from a different distribution](#2.2.-sampling-from-a-different-distribution)
    -   [Privatised Dataset](#2.3.-privatised-dataset)
    -   [Bonus Question](#2.4.-bonus-question)

# Requirements {#requirements}

```{r eval=FALSE}
install.packages('foreach') # for using for-each notation in for loops
install.packages('doParallel') # for parallellisation
install.packages('VGAM') # for sampling from a laplace
install.packages('dplyr') # for using group by on the data from the simulation
install.packages('colorspace') # for plotting
install.packages('latex2exp') # for mathematical expressions in plots
install.packages('iterators')
```
```{r echo=FALSE}
library('foreach') 
library('doParallel')
library('VGAM') 
library('dplyr') 
library('colorspace') 
library('latex2exp') 
library('iterators')

registerDoParallel(cores = detectCores())
```

# 1. Stat4Race {#1.-stat4race}

### Simulation function:

Given as input a population size `M`, the `sim` function simulates `M` times $T \sim |\{Y_1..Y_n, \ Y_n > X\}|$. At first it generates a uniform vector of size $M$, which will be $X$, then it samples $M \times b$ uniform values and puts them in a matrix, where $b$ is the base step. At this iteration $b$ values are generated for each of the $M$ individuals in the population, and if the $j$-th individual contains a value major than $X$ in the vector $M_{j,k} > X \ \ \forall k \in \{1..b\}$ then the the first k in which this happens is taken and saved in the vector `res`. Then the process is repeated through recursion for all those values that still haven't found a solution but this time with an increase of the samples extracted for each variable. So if the number of individuals that now found a solution is $M_i$ at the i-th step: 1. We sample a random uniform $(M-M_i) \times b^i$ matrix. 2. We check which of the individuals has terminated its course. 3. If there are no more survivors we stop the execution, otherwise we continue with the remaining $M_{i+1}$.

```{r}
sim <- function(X, steps.base=10, i=1){
    M = length(X)
    steps.now = min(steps.base^i, 1e7)

    y <- matrix(
        runif(steps.now * M), 
        nrow = M)
    trespass = y>X                  # for each row we see which value is over its X
    over = ifelse(
        rowSums(trespass)==0,       # if no trespasses for that row
        0,                          # then it's 0
        max.col(trespass, "first")) # else it's the first of the trespasses

    rm(y, trespass)                 # we clean the memory by releasing unused vectors
    miss.vars = which(over==0)      # we update the missing values
    miss.n = length(miss.vars)                     
    i = i + 1
    if (miss.n != 0) {
    # if there are still missing variables we repeat the process through recursion
        remainder = sim(X = X[miss.vars], i=i) + steps.now
        over[miss.vars] = remainder
    }

    return(over)
}
sim(runif(100))
```

If now want to analyze this function we can notice a few things:

-   at the beginning only a few of variables is extracted for each individual, this is because most of the individuals are short lived and will be gone after no more than 10 steps.
-   as the iterations go on the number of steps for each iteration increases exponentially $b^i$, this is because while with a large population we were restricted in the size of the steps due to memory limitations, when the population tends to decrease exponentially, we can exponentially increase the size of samples per iteration.
-   the population surviving to the next iteration is actually a bad event for our algorithm, as we have to resort to use the while loop. That's also why we want to increase the size of samples per iteration $b^i$, as doing so will guarantee us a faster extermination of our population.
-   by doing some empirical testing we can see that the number of iterations we have per each function call is usually in the order of $O(log(M))$

We can now confront the output of our simulation with the $pdf$ of T, noticing that there's a good similarity between the two:

```{r T_var, echo=FALSE}
set.seed(2008201)

T_var = sim(runif(1000))
hist(T_var, breaks = 1:1000, xlim=c(1,50), prob=T)
curve(1/x/(x+1), col='orange', lwd=3, add=T)
```

### Simulations

Finally we do the simulations, note that we split everything into 10 threads to speed up computing:

```{r S4R simulation, eval=FALSE}
set.seed(13112221)

M = 10^(2:7)
n.threads = 10
n.simulations = 1
df = data.frame()

for(i in 1:n.simulations) {
    for(m in M) {
        beg <- Sys.time()
        r = foreach(idx=1:n.threads, .combine=c) %dopar% {
            sim(runif(m/n.threads))
        }
        fin<- Sys.time() - beg

        # store results
        row = data.frame(size=m, time=fin, it=i, mean=mean(r), variance=var(r), median=median(r))
        df = rbind(df, row)
    }
}
df
```

We can then compute some summaries over our simulations:

```{r echo=FALSE}
# read summaries from simulations on Google Colab
df = read.csv('simulations.csv') 

summaries <- df %>%
  group_by(size) %>%
  dplyr::summarize(
    mean_time = mean(time),
    median_time = median(time),
    expected = mean(mean),
    variance = mean(variance),
    median = median(median)
  ) %>%
  as.data.frame()
summaries
```

This is the **mean** of the values, and as we can expect (since $\mathbb{E}T \to \infty$) it tends to grow as the size of the simulation grows:

```{r echo=FALSE, results='hide',message=FALSE}
plot(
    summaries[c('size', 'expected')],
    type = "l", 
    log='x',
    lwd=5, 
    col='green', 
    xlab='Population Size', 
    ylab='Mean', 
    main='Mean per size of input')

```

This is the time it takes, vs some linear function:

```{r echo=FALSE, results='hide',message=FALSE}
plot(
    x = summaries$size,
    y = summaries$median_time,
    type = "l", 
#    log='x',
    lwd=5, 
    col='orange', 
    xlab='Population Size', 
    ylab='time [s]', 
    main='Time per size of input')
curve(x*3.7e-6, lwd=2, add=T, type='l', lty='dotdash', from=min(df$size), to=max(df$size))
legend('topleft',
       legend = c('linear time','simulation time'),
       col = c('black', 'orange'),
       lwd=4)

```

Finally: our entry to try to win the 1kg gelato:
```{r echo=FALSE}
summaries[c('size', 'median_time')]
``` 

# 2. Differential Privacy {#2.-differential-privacy}

# 2.1. Univariate Differential Privacy {#2.1.-univariate-differential-privacy}

Index

-   [Original and Perturbed Histogram Example](#original-and-perturbed-histogram-example)
-   [Simulation](#simulation)
-   [Plotting](#plotting)

## Original and Perturbed Histogram Example {#original-and-perturbed-histogram-example}

To get a visual understanding of what it means to perturbe a histogram, we will show an original histogram sampled from a $Beta$ distribution and its perturbed version.

### Hyperparameters

```{r}
n = 1000
eps = 0.1
m = n^(1 / (2 + 1))
```

### Original Histogram

The original histogram is sampled from a $Beta$ distribution with $\alpha = 10, \beta = 10$.

```{r echo=FALSE}
b_sample = rbeta(n=n, 10, 10)

original_hist = hist(b_sample, plot=F, breaks=m-1)

plot(original_hist,
     xlab = NULL,
     ylab = NULL,
     col='red',
     border='white',
     main = paste("Histogram of", n ,"samples from a Beta(10, 10) distribution"))
```

### Perturbed Histogram {#perturbed-histogram}

Since we have to create many perturbed histograms, we will put this logic into a function for reusability.

```{r}
l_perturbe = function(h, variance = 8 / (0.001 ^ 2)) {
    perturbed_hist = h
    lap_sample = rlaplace(length(perturbed_hist$counts), scale = sqrt(variance/2))
    
    # Add the laplace sample to D_j
    perturbed_hist$counts = perturbed_hist$counts + lap_sample
    
    # If we end up with a negative number, we choose 0 instead. Same as max(0, D_j)
    perturbed_hist$counts[perturbed_hist$counts <= 0] = 0
    
    # If our samples from the laplace lead to a negative count in each bin, we cannot normalise over the
    # sum since this would be a division over 0.
    if (any(is.na(perturbed_hist$counts))) {
        perturbed_hist$counts = rep(0, length(perturbed_hist$counts))
    } else {
        # Normalise D_j over the sum of all bins
        perturbed_hist$counts = perturbed_hist$counts / sum(perturbed_hist$counts)
    }

    # Also update the density, divide q_hat by 1/m
    perturbed_hist$density = perturbed_hist$counts / (1 / m)
    
    return(perturbed_hist)
}
```

Plot the peturbed histogram. We can see that it is more sparse than the original histogram.

```{r echo=FALSE}
perturbed_hist = l_perturbe(original_hist, variance = 8 / (eps^2))
plot(original_hist, 
     freq=F, 
     main='Perturbed Histogram vs Original',
     col=rgb(1,0,0),
     border='white')
plot(perturbed_hist,
     freq=F,
     add=T, col=rgb(0,0,1,0.5),border='white')
legend('topright',
       legend=c('Original', 'Perturbed', ' Intersection'),
       col=c(rgb(1,0,0), rgb(0,0,1,0.5), rgb(.5,0,1)),
       lwd = 10
)
```

## Simulation {#simulation}

Now we can get to the actual simulation of the $MISE$ between the original distribution and original histogram, and the $MISE$ between the original distribution and the perturbed histogram.

### Hyperparameters

These are the hyperparameters needed for the simulation.

```{r}
n_values = c(100,1000) # sample size
eps_values = c(0.1, 0.001) # values for epsilon
bins_values = seq(5, 50, 5) # values for the histogram bin size
M = 1000 # number of simulations to be run
```

### Function for running simulations with fixed hyperparameters

At each simulation, we do the following:

1.  Sample from the beta distribution.
2.  Create a histogram from the sample.
3.  Perturb the histogram.
4.  Calculate the $MISE(p_X, \hat{p}_{n, m})$.
5.  Calculate the $MISE(p_X, \hat{q}_{\epsilon, m})$.
6.  Save the $MISE$ values and hyperparameters of this simulation run into a dataframe.

```{r}
mise_sim = function(n, 
                    n_bins, 
                    eps, 
                    sim_size, 
                    rdist = function(n) rbeta(n, 10, 10), 
                    dist  = function(x) dbeta(x,10,10)) {
    
    out = data.frame()
    
    for (m in 1:sim_size) {

        # 1. sample from beta
        b_sample = rdist(n=n)

        # 2. create original hist
        original_hist = hist(b_sample, plot=F, breaks=n_bins - 1)
        
        d_original = stepfun(original_hist$breaks, c(0, original_hist$density, 0))

        # 3. create perturbed hist
        perturbed_hist = l_perturbe(original_hist, variance = 8 / (eps^2))
        
        d_perturbed = stepfun(perturbed_hist$breaks, c(0, perturbed_hist$density, 0))
        
        # 4. calculate mise original
        mise_original = integrate(function(x) (dist(x) - d_original(x))^2, 0, 1, subdivisions = 1000)$value

        # 5. calculate mise perturbed
        mise_perturbed = integrate(function(x) (dist(x) - d_perturbed(x))^2, 0, 1, subdivisions = 1000)$value
        
        # 6. save the MISE values and hyperparameters
        out = rbind(out, data.frame(m=m, n=n, eps=eps, bins=n_bins, mise_original=mise_original, mise_perturbed=mise_perturbed))
    }
    
    return(out)
}

```

### Running the Simulation

We run the simulation $M$ times for all possible hyperparameter combinations. The results are saved into a dataframe.

```{r 2.1 Simulation}
sim_data = data.frame()

# Create a grid with all possible hyperparameter combinations
hyperparameter_grid = expand.grid(n = n_values, bins = bins_values, eps = eps_values)

for(rowname in rownames(hyperparameter_grid)) {
    n = hyperparameter_grid[rowname, "n"]
    bins = hyperparameter_grid[rowname, "bins"]
    eps = hyperparameter_grid[rowname, "eps"]
    
    sim_result = mise_sim(n = n, n_bins = bins, eps = eps, sim_size = M)
    sim_data = rbind(sim_data, sim_result)
}
```

## Plotting {#plotting}

### Preparing Data for Plotting

We want to group the data by epsilon value, sample size (n), and number of bins, so that we can plot the MISE values as a function of the number of bins.

```{r}

# group data
mise_data_group <- sim_data %>%
  group_by(eps, n, bins) %>%
  dplyr::summarize(mise_original_mean = mean(mise_original),
                   mise_original_min = min(mise_original),
                   mise_original_max = max(mise_original),
                   mise_perturbed_mean = mean(mise_perturbed),
                   mise_perturbed_min = min(mise_perturbed),
                   mise_perturbed_max = max(mise_perturbed)) %>% 
  as.data.frame()
```

### Function for a single plot

This function plots a line plot with two lines:

-   mean of the original MISE.
-   mean of the perturbed MISE.

```{r}
plot_mise_values = function(mise_data, main, sub, colours, lwd = 3) {
  
  max_y = max(c(mise_data$mise_original_mean, mise_data$mise_perturbed_mean))
  plot(NULL,
     xlim = c(min(bins_values), max(bins_values)),
     ylim = c(0, max_y),
     ylab = "MISE",
     xlab = "Number of bins",
     main = main,
     sub = sub)
  lines(mise_data$bins, mise_data$mise_original_mean, type = "l", lwd = lwd, col = colours[1])
  lines(mise_data$bins, mise_data$mise_perturbed_mean, type = "l", lwd = lwd, col = colours[2])
}
```

### Plotting

The 4 plots below show the:

-   $MISE$ value for the original histogram.
-   $MISE$ value for the perturbed histogram.

for the 4 combinations of the sample sizes $n \: \in \: \{100, 1000\}$ and the epsilon values $\epsilon \: \in \{0.001, 0.1\}$.

For a small privacy ratio $\epsilon = 0.001$, we can see that the $MISE$ of the perturbed histogram initially has a higher value that for a privacy ratio of $\epsilon = 0.1$. Also, a larger the sample size $n$ for the perturbed $MISE$ also corresponds to a higher initial $MISE$ when comparing values that have the same value for $\epsilon$. E.g. For $\epsilon = 0.001$, the perturbed $MISE$ for the sample size $n = 100$ starts at around $6$, whereas the perturbed $MISE$ for the sample size $n = 1000$ starts at around $10$. For small sample sizes $n = 100$, both the original $MISE$ starts off close to $0$, however with an increase in the number of bins, it approaches $1$.

One thing that is common across all 4 plots is that there is a decrease in the perturbed $MISE$ when moving from $5$ to $10$ bins. For $MISE$ that have $\epsilon = 0.1$, this initial decrease is then followed by a steady increase.

```{r echo=FALSE}

plot_mise_lines = function(mise_data, n_values, eps_values, pi = NULL) {
  line_colours = rainbow_hcl(2)
 
  mise_data_1 = subset(mise_data, mise_data$n == n_values[1] & mise_data$eps == eps_values[1])
  mise_data_2 = subset(mise_data, mise_data$n == n_values[1] & mise_data$eps == eps_values[2])
  mise_data_3 = subset(mise_data, mise_data$n == n_values[2] & mise_data$eps == eps_values[1])
  mise_data_4 = subset(mise_data, mise_data$n == n_values[2] & mise_data$eps == eps_values[2])

  # Create a layout for the 4 plots and the common legend
  layout(matrix(c(1,2,5,3,4,5), ncol=2, nrow=3), heights=c(6, 6, 2))

  main_title = paste("MISE values over", M, "Simulations.")
  subtitle1 = paste("n = ", n_values[1], "eps = ", eps_values[1])
  subtitle2 = paste("n = ", n_values[1], "eps = ", eps_values[2])
  subtitle3 = paste("n = ", n_values[2], "eps = ", eps_values[1])
  subtitle4 = paste("n = ", n_values[2], "eps = ", eps_values[2])
  # Plot the 4 plots
  if (!is.null(pi)) {
      subtitle1 = paste(subtitle1, "pi = ", pi)
      subtitle2 = paste(subtitle2, "pi = ", pi)
      subtitle3 = paste(subtitle3, "pi = ", pi)
      subtitle4 = paste(subtitle4, "pi = ", pi)
  }
  
  subtitles = c(subtitle1, subtitle2, subtitle3, subtitle4)

  plot_mise_values(mise_data_1, main_title, subtitles[1], line_colours)
  plot_mise_values(mise_data_2, main_title, subtitles[2], line_colours)
  plot_mise_values(mise_data_3, main_title, subtitles[3], line_colours)
  plot_mise_values(mise_data_4, main_title, subtitles[4], line_colours)

  # Plot the legend
  par(mai=c(0,0,0,0))
  plot.new()
  legend("center",
         legend = c(TeX("$MISE(p_X, \\hat{p}_{n,m})$"), TeX("$MISE(p_X, \\hat{q}_{\\epsilon, m})$")),
         col = line_colours,
         lwd = 3,
         xpd = TRUE,
         horiz = TRUE,
         cex = 1,
         seg.len=2)
}

plot_mise_lines(mise_data_group, n = n_values, eps = eps_values)
```

# 2.2 Sampling from a different distribution

## Index

-   [Perturbed Histogram](#perturbed-histogram)
-   [Run Simulation](#run-simulation)
-   [Plotting](#2.2.-plotting)

We now sample from: $$
p_X(x) = \pi \cdot \text{dbeta}(x | \alpha_1, \beta_1) + (1 -\pi )\cdot \text{dbeta}(x | \alpha_2, \beta_2)
$$

```{r}
# custom CDF that induces sparsity
pfunc <- function(p, pi=.5) pi * pbeta(p,20,2) + (1 - pi) * pbeta(p,2,30)
dfunc <- function(x, pi=.5) pi * dbeta(x,20,2) + (1 - pi) * dbeta(x,2,30)

# function that computes the inverse of another
inverse = function(fn, interval = NULL, lower = min(interval), upper = max(interval), ...){
    Vectorize(function(y){
        uniroot(f=function(x){fn(x)-y}, lower=lower, upper=upper, ...)$root
    })
}

# Quantile function of the custom CDF
qfunc <- inverse(pfunc, interval = c(0,1))
n.approx_size = (0:1e4)/1e4
qfunc <- approxfun(n.approx_size, qfunc(n.approx_size))
# Quantile transformation
rfunc <- function(n) qfunc(runif(n))

h=hist(rfunc(1000), plot=F)
plot(h, main='Density of
     p[X]',xlab = '', freq=F, col='red', border='white')
```

## Perturbed Histogram

```{r echo=FALSE}
h_perturbed=l_perturbe(h)

plot(h, 
     freq=F, 
     main='Perturbed Histogram vs Original',
     col=rgb(1,0,0),
     border='white', xlab='')
plot(h_perturbed,
     freq=F,
     add=T, col=rgb(0,0,1,0.5),border='white')
legend('topright',
       legend=c('Original', 'Perturbed', ' Intersection'),
       col=c(rgb(1,0,0), rgb(0,0,1,0.5), rgb(.5,0,1)),
       lwd = 10)
```

## Run Simulation {#run-simulation}

We will run the simulation with the same parameters for $n$, $\epsilon$ and $m$ and for the same number of times $M$. We also have $\pi = 0.5$ as we noted that changing it didn't gave much of an effect on the results.

```{r 2.2 Simulation}
out_new = data.frame()

hyperparameter_grid = expand.grid(n = n_values, bins = bins_values, eps = eps_values)

for (n in n_values) {
    for (eps in eps_values) {
        for (n_bins in bins_values) {
              sim_result = mise_sim(
                n, 
                n_bins, 
                eps, 
                sim_size = M, 
                rdist= rfunc,
                dist = dfunc)
              out_new = rbind(out_new, sim_result)
          }
    }
}
```

## Plotting {#2.2.-plotting}

Like in the plots for question 2.1, we will show how the two calculated $MISE$ values vary across the different combinations of hyper parameters.

### Group Data

First, we group the data, however this time we also group on $\pi$ in addition to the other parameters.

```{r}
# group data
mise_data_group_pi <- out_new %>%
  group_by(eps, n, bins) %>%
  dplyr::summarize(mise_original_mean = mean(mise_original),
                   mise_original_min = min(mise_original),
                   mise_original_max = max(mise_original),
                   mise_perturbed_mean = mean(mise_perturbed),
                   mise_perturbed_min = min(mise_perturbed),
                   mise_perturbed_max = max(mise_perturbed)) %>% 
  as.data.frame()
```

### Subset Data

Finally, we create one subset of the data for each value of $\pi$. We do this because splitting the data across three plots, each with their own $\pi$, makes the visualisation more readable.

### Plot pi = 0.5.

```{r echo=FALSE}
pi_05 = subset(mise_data_group_pi, mise_data_group_pi$pi == 0.5)
plot_mise_lines(pi_05, n_values, eps_values, pi = 0.5)
```

## 2.4 Bonus

We want to perform some simulations to test whether our mechanism ensures $\epsilon$ differential privacy:

```{r}
n.sim = 100
ratio = numeric(n.sim)
eps = .1

for (i in 1:n.sim) {
  h = hist(rbeta(1e5,10,10), breaks=floor(1e5^(1/3)),plot=F)
  h$density = h$density/2
  h_p = l_perturbe(h,variance=8/eps^2)

  diff = max( h$density[h_p$density!=0]/h_p$density[h_p$density!=0] )
  #plot(h, freq=F, c=rgb(1,0,0))
  #plot(h_p, freq=F, add=T, c= rgb(0,1,0,0.5))
  ratio[i] = diff
}
sum(ratio <= exp(eps)) / n.sim
```
As we can see the differential privacy is ensured most of the times.
